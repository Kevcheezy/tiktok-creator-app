# R1.3 Reference Video Intelligence — Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build a VideoAnalysisAgent that downloads reference TikTok videos via yt-dlp, analyzes them with Google Gemini 2.5 Flash using the SEAL method (Scene, Emotion, Angle, Lighting), and strongly feeds the structured analysis into ScriptingAgent and CastingAgent.

**Architecture:** VideoAnalysisAgent extends BaseAgent and runs in parallel with ProductAnalyzerAgent during the `analyzing` pipeline step. It uses yt-dlp (child_process) to download videos, Supabase Storage to host them, and a new GeminiClient (`@google/generative-ai` SDK) for multimodal video analysis. The SEAL breakdown is stored as `video_analysis` JSONB on the `project` table and consumed by ScriptingAgent (prompt enrichment) and CastingAgent (keyframe prompt enrichment). Frontend shows reference video player and SEAL cards on the analysis review page.

**Tech Stack:** Next.js 16 (TypeScript), Supabase (PostgreSQL + Storage), BullMQ, Google Gemini 2.5 Flash (`@google/generative-ai`), yt-dlp (CLI)

---

## Task 1: Database Migration — Add `video_analysis` Column

**Files:**
- None (migration via Supabase MCP)
- Modify: `src/db/schema.ts:119` — add column definition

**Step 1: Apply migration via Supabase MCP**

```sql
ALTER TABLE project ADD COLUMN video_analysis jsonb;
```

Migration name: `add_video_analysis_column`

**Step 2: Update schema.ts**

In `src/db/schema.ts`, inside the `project` table definition, after the `videoUrl` line (line 119), add:

```typescript
  videoAnalysis: jsonb('video_analysis'),
```

**Step 3: Verify**

Run: `npm run build`
Expected: Compiles cleanly

**Step 4: Commit**

```bash
git add src/db/schema.ts
git commit -m "feat(r1.3): add video_analysis JSONB column to project table"
git push
```

---

## Task 2: Install Dependencies — `@google/generative-ai`

**Files:**
- Modify: `package.json`

**Step 1: Install the package**

```bash
npm install @google/generative-ai
```

**Step 2: Verify**

Run: `npm run build`
Expected: Compiles cleanly, package in `node_modules`

**Step 3: Commit**

```bash
git add package.json package-lock.json
git commit -m "feat(r1.3): add @google/generative-ai dependency"
git push
```

---

## Task 3: Create GeminiClient API Client

**Files:**
- Create: `src/lib/api-clients/gemini.ts`

**Context:** This client wraps the `@google/generative-ai` SDK to upload a video file and send it to Gemini 2.5 Flash for multimodal analysis. Pattern matches the existing `WaveSpeedClient` at `src/lib/api-clients/wavespeed.ts` — constructor takes optional API key, uses structured pino logging, includes timing and error logging.

**Step 1: Create the client file**

Create `src/lib/api-clients/gemini.ts`:

```typescript
import { GoogleGenerativeAI, FileState } from '@google/generative-ai';
import { GoogleAIFileManager } from '@google/generative-ai/server';
import { createLogger } from '@/lib/logger';

const logger = createLogger({ agentName: 'GeminiClient' });

export class GeminiClient {
  private genAI: GoogleGenerativeAI;
  private fileManager: GoogleAIFileManager;
  private model: string;

  constructor(apiKey?: string, model = 'gemini-2.5-flash') {
    const key = apiKey || process.env.GOOGLE_API_KEY || '';
    if (!key) {
      logger.warn('No GOOGLE_API_KEY provided');
    }
    this.genAI = new GoogleGenerativeAI(key);
    this.fileManager = new GoogleAIFileManager(key);
    this.model = model;
  }

  /**
   * Upload a local video file to Gemini's File API,
   * wait for it to become ACTIVE, then run generateContent
   * with the video + text prompts.
   */
  async analyzeVideo(
    filePath: string,
    systemPrompt: string,
    userPrompt: string,
    options?: { temperature?: number; maxTokens?: number }
  ): Promise<string> {
    const start = Date.now();

    // 1. Upload the video file
    logger.info({ filePath }, 'Uploading video to Gemini File API');
    const uploadResult = await this.fileManager.uploadFile(filePath, {
      mimeType: 'video/mp4',
      displayName: filePath.split('/').pop() || 'reference-video',
    });

    let file = uploadResult.file;
    logger.info({ fileName: file.name, state: file.state }, 'File uploaded, waiting for processing');

    // 2. Poll until file is ACTIVE (processed)
    const pollStart = Date.now();
    const maxPollMs = 120_000; // 2 minutes max
    while (file.state === FileState.PROCESSING) {
      if (Date.now() - pollStart > maxPollMs) {
        throw new Error(`Gemini file processing timed out after ${maxPollMs / 1000}s`);
      }
      await new Promise((resolve) => setTimeout(resolve, 5000));
      const result = await this.fileManager.getFile(file.name);
      file = result;
    }

    if (file.state === FileState.FAILED) {
      throw new Error(`Gemini file processing failed for ${file.name}`);
    }

    logger.info({ fileName: file.name, processingMs: Date.now() - pollStart }, 'File ready for analysis');

    // 3. Send to generateContent with video + text
    const model = this.genAI.getGenerativeModel({
      model: this.model,
      systemInstruction: systemPrompt,
    });

    const result = await model.generateContent([
      {
        fileData: {
          mimeType: file.mimeType,
          fileUri: file.uri,
        },
      },
      { text: userPrompt },
    ]);

    const responseText = result.response.text();
    const totalMs = Date.now() - start;
    logger.info({ totalMs, responseLength: responseText.length }, 'Video analysis complete');

    // 4. Clean up the uploaded file (fire-and-forget)
    this.fileManager.deleteFile(file.name).catch((err) => {
      logger.warn({ fileName: file.name, err }, 'Failed to delete uploaded file from Gemini');
    });

    return responseText;
  }
}
```

**Step 2: Verify**

Run: `npm run build`
Expected: Compiles cleanly

**Step 3: Commit**

```bash
git add src/lib/api-clients/gemini.ts
git commit -m "feat(r1.3): add GeminiClient for multimodal video analysis"
git push
```

---

## Task 4: Add Gemini Cost Constant

**Files:**
- Modify: `src/lib/constants.ts:392-399`

**Step 1: Add cost entry**

In `src/lib/constants.ts`, inside the `API_COSTS` object, add after the `creatomateRender` line:

```typescript
  geminiVideoAnalysis: 0.02,
```

So the full object becomes:

```typescript
export const API_COSTS = {
  wavespeedChat: 0.01,
  nanoBananaPro: 0.07,
  nanoBananaProEdit: 0.07,
  klingVideo: 1.20,
  elevenLabsTts: 0.05,
  creatomateRender: 0.50,
  geminiVideoAnalysis: 0.02,
} as const;
```

**Step 2: Verify**

Run: `npm run build`
Expected: Compiles cleanly

**Step 3: Commit**

```bash
git add src/lib/constants.ts
git commit -m "feat(r1.3): add geminiVideoAnalysis cost constant"
git push
```

---

## Task 5: Create VideoAnalysisAgent

**Files:**
- Create: `src/agents/video-analysis-agent.ts`

**Context:** This agent extends `BaseAgent` (see `src/agents/base-agent.ts`). Pattern matches `ProductAnalyzerAgent` at `src/agents/product-analyzer.ts`. It:
1. Reads `project.video_url` from DB
2. Downloads video via `yt-dlp` (child_process.execFile)
3. Uploads to Supabase Storage
4. Sends to GeminiClient for SEAL analysis
5. Parses JSON response
6. Stores `video_analysis` on the project
7. Cleans up temp file

The SEAL (Scene, Emotion, Angle, Lighting) method structures per-segment analysis optimized for Nano Banana Pro keyframe generation prompts.

**Step 1: Create the agent file**

Create `src/agents/video-analysis-agent.ts`:

```typescript
import { SupabaseClient } from '@supabase/supabase-js';
import { execFile } from 'child_process';
import { readFile, unlink, stat } from 'fs/promises';
import { BaseAgent } from './base-agent';
import { GeminiClient } from '@/lib/api-clients/gemini';
import { API_COSTS } from '@/lib/constants';

export interface VideoAnalysis {
  hook: {
    type: string;
    technique: string;
    text: string;
    durationSeconds: number;
  };
  segments: Array<{
    index: number;
    startTime: number;
    endTime: number;
    scene: {
      setting: string;
      props: string[];
      composition: string;
      productPresence: string;
    };
    emotion: {
      mood: string;
      energy: string;
      pacing: string;
      viewerIntent: string;
    };
    angle: {
      shotType: string;
      cameraMovement: string;
      transitions: string;
    };
    lighting: {
      style: string;
      colorTemp: string;
      contrast: string;
    };
    description: string;
  }>;
  overall: {
    energyArc: string;
    dominantStyle: string;
    musicPresence: boolean;
    textOverlayStyle: string;
    estimatedDuration: number;
    viralPattern: string;
  };
}

const SYSTEM_PROMPT = `You are an expert TikTok video analyst specializing in the SEAL method (Scene, Emotion, Angle, Lighting) for breaking down viral video content.

You will receive a reference TikTok video. Analyze it thoroughly and output a structured JSON breakdown following the SEAL framework. This data will be used to generate AI keyframe images via Nano Banana Pro and to guide AI script generation — so be precise and visual in your descriptions.

You MUST respond with valid JSON only. No markdown, no code fences, no explanation text. Just the raw JSON object.

The JSON must have exactly this structure:
{
  "hook": {
    "type": "question | shocking-stat | controversy | curiosity-gap | before-after | direct-address",
    "technique": "Description of what makes the hook effective",
    "text": "Transcribed or described hook text/action",
    "durationSeconds": 3
  },
  "segments": [
    {
      "index": 0,
      "startTime": 0,
      "endTime": 15,
      "scene": {
        "setting": "Specific location description (e.g. 'modern white kitchen with marble countertops')",
        "props": ["Array of visible props and objects"],
        "composition": "How the frame is composed (e.g. 'subject center-frame, product on counter in foreground')",
        "productPresence": "none | subtle | prominent | hero"
      },
      "emotion": {
        "mood": "Primary emotional state of the presenter (e.g. 'excited', 'skeptical', 'surprised')",
        "energy": "low | medium | high | peak",
        "pacing": "slow | moderate | fast | rapid",
        "viewerIntent": "What the viewer should feel (curiosity | trust | desire | urgency)"
      },
      "angle": {
        "shotType": "close-up | medium | wide | extreme-close-up | product-insert",
        "cameraMovement": "static | slow-zoom-in | pan | handheld | tracking",
        "transitions": "hard-cut | smooth | jump-cut | swipe"
      },
      "lighting": {
        "style": "natural-window | ring-light | studio | dramatic | golden-hour",
        "colorTemp": "warm | cool | neutral",
        "contrast": "soft | medium | high"
      },
      "description": "Brief narrative of what happens in this segment"
    }
  ],
  "overall": {
    "energyArc": "build-to-peak | peak-valley-peak | steady-high | slow-burn",
    "dominantStyle": "Free-text summary of the video's visual identity and aesthetic",
    "musicPresence": true,
    "textOverlayStyle": "none | minimal-captions | bold-headlines | animated",
    "estimatedDuration": 60,
    "viralPattern": "What specific pattern makes this video likely to go viral"
  }
}

RULES:
- segments array MUST have exactly 4 entries (index 0-3), each covering ~15 seconds
- All enum values must match the options listed above exactly
- scene.setting should be specific enough to generate an AI image (not "a room" but "modern white kitchen with marble countertops")
- scene.props should list 3-5 visible items
- scene.composition should describe framing useful for image generation
- emotion.mood should be a single descriptive word
- If the video is shorter than 60 seconds, still divide into 4 equal segments
- If the video is a different format, adapt the SEAL analysis to what's visible`;

const USER_PROMPT = `Analyze this reference TikTok video using the SEAL method (Scene, Emotion, Angle, Lighting).

Break it into exactly 4 segments of roughly equal duration. For each segment, provide detailed SEAL data that could be used to:
1. Generate matching keyframe images via an AI image model
2. Write a script that follows the same structure and energy arc
3. Direct video generation that matches the visual style

Be specific and visual in all descriptions — these will be used as AI generation prompts.`;

const MAX_VIDEO_SIZE_BYTES = 100 * 1024 * 1024; // 100MB
const DOWNLOAD_TIMEOUT_MS = 60_000; // 60s

export class VideoAnalysisAgent extends BaseAgent {
  private gemini: GeminiClient;

  constructor(supabaseClient?: SupabaseClient) {
    super('VideoAnalysisAgent', supabaseClient);
    this.gemini = new GeminiClient();
  }

  async run(projectId: string): Promise<VideoAnalysis | null> {
    const stageStart = Date.now();
    await this.logEvent(projectId, 'stage_start', 'video_analysis');
    this.log(`Starting video analysis for project ${projectId}`);

    // 1. Fetch project
    const { data: project, error } = await this.supabase
      .from('project')
      .select('video_url')
      .eq('id', projectId)
      .single();

    if (error || !project?.video_url) {
      this.log('No video URL found, skipping video analysis');
      return null;
    }

    const videoUrl = project.video_url;
    const tmpPath = `/tmp/${projectId}-reference.mp4`;

    try {
      // 2. Download video via yt-dlp
      this.log(`Downloading video from ${videoUrl}`);
      await this.downloadVideo(videoUrl, tmpPath);

      // 3. Check file size
      const fileStat = await stat(tmpPath);
      this.log(`Video downloaded: ${(fileStat.size / 1024 / 1024).toFixed(1)}MB`);
      if (fileStat.size > MAX_VIDEO_SIZE_BYTES) {
        throw new Error(`Video too large: ${(fileStat.size / 1024 / 1024).toFixed(1)}MB (max ${MAX_VIDEO_SIZE_BYTES / 1024 / 1024}MB)`);
      }

      // 4. Upload to Supabase Storage
      const storagePath = `projects/${projectId}/reference.mp4`;
      const videoBuffer = await readFile(tmpPath);
      const { error: uploadError } = await this.supabase.storage
        .from('assets')
        .upload(storagePath, videoBuffer, {
          contentType: 'video/mp4',
          upsert: true,
        });

      if (uploadError) {
        this.log(`Storage upload warning: ${uploadError.message} (continuing with local file)`);
      } else {
        const { data: urlData } = this.supabase.storage.from('assets').getPublicUrl(storagePath);
        this.log(`Video uploaded to storage: ${urlData.publicUrl}`);

        // Store the reference video URL on the project
        await this.supabase
          .from('project')
          .update({ render_url: urlData.publicUrl })
          .eq('id', projectId);
      }

      // 5. Send to Gemini for SEAL analysis
      this.log('Sending video to Gemini for SEAL analysis');
      const rawResponse = await this.gemini.analyzeVideo(
        tmpPath,
        SYSTEM_PROMPT,
        USER_PROMPT,
        { temperature: 0.3 }
      );
      await this.trackCost(projectId, API_COSTS.geminiVideoAnalysis);

      // 6. Parse and validate response
      const cleaned = rawResponse
        .replace(/```json\n?/g, '')
        .replace(/```\n?/g, '')
        .trim();

      const analysis: VideoAnalysis = JSON.parse(cleaned);

      // Basic validation
      if (!analysis.hook || !analysis.segments || !analysis.overall) {
        throw new Error('Invalid video analysis response: missing required fields');
      }
      if (analysis.segments.length !== 4) {
        this.log(`Warning: Expected 4 segments, got ${analysis.segments.length}`);
      }

      // 7. Store on project
      await this.supabase
        .from('project')
        .update({
          video_analysis: analysis,
          updated_at: new Date().toISOString(),
        })
        .eq('id', projectId);

      const durationMs = Date.now() - stageStart;
      await this.logEvent(projectId, 'stage_complete', 'video_analysis', { durationMs });
      this.log(`Video analysis complete in ${durationMs}ms`);

      return analysis;
    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : String(err);
      const durationMs = Date.now() - stageStart;
      this.log(`Video analysis failed: ${errorMessage}`, { durationMs });
      await this.logEvent(projectId, 'stage_error', 'video_analysis', {
        error: errorMessage,
        durationMs,
      });

      // Non-blocking: set video_analysis to null, don't fail the pipeline
      await this.supabase
        .from('project')
        .update({
          video_analysis: null,
          updated_at: new Date().toISOString(),
        })
        .eq('id', projectId);

      return null;
    } finally {
      // Clean up temp file
      await unlink(tmpPath).catch(() => {});
    }
  }

  private downloadVideo(url: string, outputPath: string): Promise<void> {
    return new Promise((resolve, reject) => {
      const args = [
        url,
        '-o', outputPath,
        '-f', 'best[ext=mp4][height<=720]/best[ext=mp4]/best',
        '--no-playlist',
        '--no-warnings',
        '--quiet',
      ];

      const proc = execFile('yt-dlp', args, { timeout: DOWNLOAD_TIMEOUT_MS }, (error) => {
        if (error) {
          reject(new Error(`yt-dlp failed: ${error.message}`));
        } else {
          resolve();
        }
      });

      // Safety: kill on timeout
      setTimeout(() => {
        proc.kill('SIGTERM');
      }, DOWNLOAD_TIMEOUT_MS + 5000);
    });
  }
}
```

**Step 2: Verify**

Run: `npm run build`
Expected: Compiles cleanly

**Step 3: Commit**

```bash
git add src/agents/video-analysis-agent.ts
git commit -m "feat(r1.3): add VideoAnalysisAgent with yt-dlp download and Gemini SEAL analysis"
git push
```

---

## Task 6: Integrate VideoAnalysisAgent into Pipeline Worker

**Files:**
- Modify: `src/workers/pipeline.worker.ts:7,82-227`

**Context:** The `handleProductAnalysis` function in the pipeline worker currently only runs `ProductAnalyzerAgent`. We need to run `VideoAnalysisAgent` in parallel when the project has a `video_url`. If video analysis fails, log a warning but don't fail the pipeline (product analysis failure still fails the pipeline).

**Step 1: Add import**

At the top of `src/workers/pipeline.worker.ts`, after the EditorAgent import (line 12), add:

```typescript
import { VideoAnalysisAgent } from '../agents/video-analysis-agent';
```

**Step 2: Modify `handleProductAnalysis` — standard project-linked analysis path**

In the `handleProductAnalysis` function, find the standard project-linked analysis section (starts around line 145 with the comment `// Standard project-linked analysis`). Currently it runs only `ProductAnalyzerAgent`. Modify it to also run `VideoAnalysisAgent` in parallel when `video_url` is present.

Replace the try block contents (lines 146-226) with:

```typescript
  // Standard project-linked analysis
  try {
    await supabase
      .from('project')
      .update({ status: 'analyzing', updated_at: new Date().toISOString() })
      .eq('id', projectId);

    await logToGenerationLog(supabase, {
      project_id: projectId!, correlation_id: correlationId,
      event_type: 'stage_start', agent_name: 'ProductAnalyzerAgent', stage,
    });

    // Check if project has a video URL for parallel video analysis
    const { data: projCheck } = await supabase
      .from('project')
      .select('video_url')
      .eq('id', projectId)
      .single();

    const agent = new ProductAnalyzerAgent(supabase);
    agent.setCorrelationId(correlationId);

    // Build parallel tasks: product analysis + optional video analysis
    const productAnalysisPromise = agent.run(projectId!);

    let videoAnalysisPromise: Promise<unknown> | null = null;
    if (projCheck?.video_url) {
      jobLog.info('Video URL detected, running VideoAnalysisAgent in parallel');
      const videoAgent = new VideoAnalysisAgent(supabase);
      videoAgent.setCorrelationId(correlationId);
      videoAnalysisPromise = videoAgent.run(projectId!);
    }

    // Run in parallel with Promise.allSettled
    const tasks: Promise<unknown>[] = [productAnalysisPromise];
    if (videoAnalysisPromise) tasks.push(videoAnalysisPromise);

    const results = await Promise.allSettled(tasks);

    // Product analysis failure = pipeline failure
    if (results[0].status === 'rejected') {
      throw results[0].reason;
    }
    const analysis = (results[0] as PromiseFulfilledResult<any>).value;

    // Video analysis failure = log warning, continue
    if (results.length > 1 && results[1].status === 'rejected') {
      jobLog.warn({ err: results[1].reason }, 'Video analysis failed (non-blocking), continuing pipeline');
    }

    // Write to product table if project has a product_id
    const { data: proj } = await supabase
      .from('project')
      .select('product_id')
      .eq('id', projectId)
      .single();

    if (proj?.product_id) {
      await writeAnalysisToProduct(proj.product_id, analysis, jobLog);
    }

    // Write denormalized fields to project (backward compat)
    const updateData: Record<string, unknown> = {
      status: 'analysis_review',
      product_data: analysis,
      product_name: analysis.product_name,
      product_category: analysis.category,
      updated_at: new Date().toISOString(),
    };
    if (analysis.product_image_url) {
      updateData.product_image_url = analysis.product_image_url;
    }

    await supabase
      .from('project')
      .update(updateData)
      .eq('id', projectId);

    const durationMs = Date.now() - stageStart;
    await logToGenerationLog(supabase, {
      project_id: projectId!, correlation_id: correlationId,
      event_type: 'stage_complete', agent_name: 'ProductAnalyzerAgent', stage,
      detail: { durationMs, videoAnalysisRan: !!videoAnalysisPromise },
    });
    jobLog.info({ durationMs, videoAnalysisRan: !!videoAnalysisPromise }, 'Product analysis complete');
  } catch (error) {
    // ... existing error handling unchanged ...
```

The catch block and everything below stays the same.

**Step 3: Verify**

Run: `npm run build`
Expected: Compiles cleanly

**Step 4: Commit**

```bash
git add src/workers/pipeline.worker.ts
git commit -m "feat(r1.3): run VideoAnalysisAgent in parallel during product_analysis pipeline step"
git push
```

---

## Task 7: Integrate SEAL Data into ScriptingAgent

**Files:**
- Modify: `src/agents/scripting-agent.ts:150-200,793-839`

**Context:** The ScriptingAgent currently passes `video_url` as a raw URL string in the LLM prompt (line 828-831). When `video_analysis` JSONB exists on the project, replace the raw URL with a rich SEAL-structured block that strongly influences the script structure.

**Step 1: Update the `run()` method to fetch `video_analysis`**

In the `run()` method (around line 156), the project select query is:
```typescript
.select('*, character:ai_character(*)')
```

Change it to also fetch `video_analysis`:
```typescript
.select('*, character:ai_character(*), video_analysis')
```

Wait — `video_analysis` is a column on the project table, so `.select('*')` already includes it. The `*` wildcard fetches all columns. So no change is needed to the select query — `proj.video_analysis` will already be available.

**Step 2: Update `buildUserPrompt` to accept `videoAnalysis`**

Modify the `buildUserPrompt` method signature (around line 793) to accept the analysis:

Change:
```typescript
  private buildUserPrompt(
    productData: {
      product_name: string;
      category: string;
      selling_points: string[];
      hook_angle: string;
    },
    template: {
      hook_type: string;
      text_hook_template: string;
      spoken_hook_template: string;
      energy_arc: unknown;
    } | null,
    videoUrl?: string | null
  ): string {
```

To:
```typescript
  private buildUserPrompt(
    productData: {
      product_name: string;
      category: string;
      selling_points: string[];
      hook_angle: string;
    },
    template: {
      hook_type: string;
      text_hook_template: string;
      spoken_hook_template: string;
      energy_arc: unknown;
    } | null,
    videoUrl?: string | null,
    videoAnalysis?: any | null,
  ): string {
```

**Step 3: Replace the `videoUrl` prompt section**

In `buildUserPrompt`, replace lines 828-836:

```typescript
    if (videoUrl) {
      prompt += `

REFERENCE VIDEO (analyze structure): ${videoUrl}`;
    } else {
      prompt += `

MODE: Generate from scratch using proven hook formula`;
    }
```

With:

```typescript
    if (videoAnalysis) {
      // SEAL-structured reference data — strong influence on script
      const sealBlock = this.buildSEALBlock(videoAnalysis);
      prompt += `

${sealBlock}`;
    } else if (videoUrl) {
      prompt += `

REFERENCE VIDEO (analyze structure): ${videoUrl}`;
    } else {
      prompt += `

MODE: Generate from scratch using proven hook formula`;
    }
```

**Step 4: Add the `buildSEALBlock` helper method**

Add this method to the ScriptingAgent class, after `buildUserPrompt`:

```typescript
  private buildSEALBlock(analysis: any): string {
    const hook = analysis.hook;
    const segments = analysis.segments || [];
    const overall = analysis.overall;

    let block = `REFERENCE VIDEO ANALYSIS (STRONG INFLUENCE — match this video's style):

HOOK: ${hook?.type || 'unknown'} — ${hook?.technique || 'N/A'}
Hook text: "${hook?.text || 'N/A'}" (${hook?.durationSeconds || 3}s)
Overall energy arc: ${overall?.energyArc || 'build-to-peak'}
Dominant style: ${overall?.dominantStyle || 'N/A'}
Viral pattern: ${overall?.viralPattern || 'N/A'}
Text overlays: ${overall?.textOverlayStyle || 'none'}`;

    for (const seg of segments) {
      block += `

SEGMENT ${seg.index + 1} (${seg.startTime}s-${seg.endTime}s):
  Scene: ${seg.scene?.setting || 'N/A'} — ${seg.scene?.composition || 'N/A'}
  Props: ${(seg.scene?.props || []).join(', ') || 'none'}
  Product: ${seg.scene?.productPresence || 'none'}
  Emotion: ${seg.emotion?.mood || 'neutral'} mood, ${seg.emotion?.energy || 'medium'} energy, ${seg.emotion?.pacing || 'moderate'} pacing
  Viewer intent: ${seg.emotion?.viewerIntent || 'curiosity'}
  Camera: ${seg.angle?.shotType || 'medium'} shot, ${seg.angle?.cameraMovement || 'static'}
  Transitions: ${seg.angle?.transitions || 'hard-cut'}
  Lighting: ${seg.lighting?.style || 'natural'}, ${seg.lighting?.colorTemp || 'neutral'} temp, ${seg.lighting?.contrast || 'medium'} contrast
  What happens: ${seg.description || 'N/A'}`;
    }

    block += `

INSTRUCTION: Your script MUST strongly match this reference video's:
- Hook style and opening technique (use "${hook?.type || 'similar'}" hook approach)
- Energy arc and pacing progression (${overall?.energyArc || 'build-to-peak'})
- Segment structure and timing
- Product reveal pattern (${segments.map((s: any) => s.scene?.productPresence || 'none').join(' → ')})
Adapt the CONTENT for the current product while preserving the reference's proven viral STRUCTURE and ENERGY.`;

    return block;
  }
```

**Step 5: Update the `buildUserPrompt` call site**

In the `run()` method (around line 187), update the call:

Change:
```typescript
    const userPrompt = this.buildUserPrompt(productData, template, proj.video_url);
```

To:
```typescript
    const userPrompt = this.buildUserPrompt(productData, template, proj.video_url, proj.video_analysis);
```

**Step 6: Verify**

Run: `npm run build`
Expected: Compiles cleanly

**Step 7: Commit**

```bash
git add src/agents/scripting-agent.ts
git commit -m "feat(r1.3): integrate SEAL video analysis into ScriptingAgent prompt"
git push
```

---

## Task 8: Integrate SEAL Data into CastingAgent

**Files:**
- Modify: `src/agents/casting-agent.ts:14-27,191-258`

**Context:** The CastingAgent generates visual prompts for Nano Banana Pro keyframe generation. When `video_analysis` exists on the project, enrich the `generateVisualPrompts` call with SEAL data for each segment so keyframes visually match the reference video's aesthetic.

**Step 1: Fetch `video_analysis` in the `run()` method**

The project select query (around line 21) already fetches `*` which includes `video_analysis`. So `project.video_analysis` is already available.

**Step 2: Pass SEAL data to `generateVisualPrompts`**

In the segment loop (around line 104), update the `generateVisualPrompts` call to also pass SEAL data for the current segment:

Change:
```typescript
          const promptPair = await this.generateVisualPrompts(
            appearance, wardrobe, setting,
            scene, placement, energyArc,
            project.product_name || 'the product',
            projectId,
            useInfluencer,
          );
```

To:
```typescript
          const sealSegment = project.video_analysis?.segments?.[segIdx] || null;
          const promptPair = await this.generateVisualPrompts(
            appearance, wardrobe, setting,
            scene, placement, energyArc,
            project.product_name || 'the product',
            projectId,
            useInfluencer,
            sealSegment,
          );
```

**Step 3: Update `generateVisualPrompts` to accept and use SEAL data**

Update the method signature (around line 191):

Change:
```typescript
  private async generateVisualPrompts(
    appearance: string,
    wardrobe: string,
    setting: string,
    scene: any,
    placement: { section: string; visibility: string; description: string },
    energyArc: (typeof ENERGY_ARC)[number],
    productName: string,
    projectId: string,
    isEdit: boolean = false,
  ): Promise<{ start: string; end: string }> {
```

To:
```typescript
  private async generateVisualPrompts(
    appearance: string,
    wardrobe: string,
    setting: string,
    scene: any,
    placement: { section: string; visibility: string; description: string },
    energyArc: (typeof ENERGY_ARC)[number],
    productName: string,
    projectId: string,
    isEdit: boolean = false,
    sealData?: any | null,
  ): Promise<{ start: string; end: string }> {
```

**Step 4: Enrich the LLM prompts with SEAL data**

In the `generateVisualPrompts` method, after the existing `userPrompt` construction for both `isEdit` and non-edit branches, append SEAL context if available.

After the line that starts `const userPrompt = isEdit` (around line 212) and before `const response = await this.wavespeed.chatCompletion(...)` (around line 241), add:

```typescript
    // Enrich with SEAL reference data if available
    let enrichedUserPrompt = userPrompt;
    if (sealData) {
      enrichedUserPrompt += `

REFERENCE VIDEO SEAL DATA (match this visual style):
  Scene: ${sealData.scene?.setting || 'N/A'}, ${sealData.scene?.composition || 'N/A'}
  Props: ${(sealData.scene?.props || []).join(', ') || 'none'}
  Shot type: ${sealData.angle?.shotType || 'medium'}, Camera: ${sealData.angle?.cameraMovement || 'static'}
  Lighting: ${sealData.lighting?.style || 'natural'}, ${sealData.lighting?.colorTemp || 'neutral'} temp, ${sealData.lighting?.contrast || 'medium'} contrast
  Mood: ${sealData.emotion?.mood || 'neutral'}

Match this reference video's visual style: use similar lighting, camera angle, and composition.`;
    }
```

Then update the chatCompletion call to use `enrichedUserPrompt`:

Change:
```typescript
    const response = await this.wavespeed.chatCompletion(systemPrompt, userPrompt);
```

To:
```typescript
    const response = await this.wavespeed.chatCompletion(systemPrompt, enrichedUserPrompt);
```

**Step 5: Verify**

Run: `npm run build`
Expected: Compiles cleanly

**Step 6: Commit**

```bash
git add src/agents/casting-agent.ts
git commit -m "feat(r1.3): enrich CastingAgent keyframe prompts with SEAL reference data"
git push
```

---

## Task 9: Frontend — Reference Video Analysis on Analysis Review Page

**Files:**
- Modify: `src/components/project-detail.tsx`

**Context:** The project detail page displays different UI based on project status. In the `analysis_review` state, show the reference video player and SEAL breakdown cards when `video_analysis` exists on the project. This requires using the `frontend-designer` skill.

**Note:** This task MUST use the `frontend-designer` skill per CLAUDE.md rules. The implementer should invoke `/frontend-designer` before touching any `.tsx` files.

**Step 1: Read the existing `project-detail.tsx`**

Read the full file to understand the current `analysis_review` state rendering.

**Step 2: Add reference video section to analysis_review state**

When the project has `video_analysis` and a reference video URL (stored in `render_url` after upload, or reconstructable from project ID), render:

1. A section header: "Reference Video Analysis"
2. An HTML5 `<video>` player with the reference video
3. A grid of 4 SEAL breakdown cards (one per segment) showing:
   - Segment number and time range
   - Scene setting and composition
   - Emotion/energy/pacing
   - Camera angle and movement
   - Lighting style
4. Overall analysis card showing hook type, energy arc, viral pattern

The cards should match the app's existing dark cinematic aesthetic with `bg-surface`, `border-border`, `text-text-primary/secondary/muted`, and neon accent colors.

**Step 3: Verify**

Run: `npm run build`
Expected: Compiles cleanly

**Step 4: Commit**

```bash
git add src/components/project-detail.tsx
git commit -m "feat(r1.3): add reference video analysis display on analysis review page"
git push
```

---

## Task 10: Frontend — Reference vs Generated Comparison on Completed Page

**Files:**
- Modify: `src/components/project-detail.tsx`

**Context:** In the `completed` state of the project detail page, show a "Reference vs Generated" comparison section when the project used a reference video. Display the reference video alongside the final generated video.

**Note:** This task MUST use the `frontend-designer` skill per CLAUDE.md rules.

**Step 1: Add comparison section to completed state**

When `project.video_analysis` exists and the project has a final video:

1. Add a section below the main video player: "Reference Comparison"
2. Side-by-side layout:
   - Left: Reference video player (from Supabase Storage)
   - Right: Generated video player (existing final video)
3. Below the comparison: a summary of which SEAL elements were matched

**Step 2: Verify**

Run: `npm run build`
Expected: Compiles cleanly

**Step 3: Commit**

```bash
git add src/components/project-detail.tsx
git commit -m "feat(r1.3): add reference vs generated comparison on completed page"
git push
```

---

## Task 11: Update Roadmap + Final Build Verification

**Files:**
- Modify: `docs/PRODUCT_ROADMAP.md:152-161`

**Step 1: Update roadmap**

Mark all R1.3 items as complete with checkboxes:

```markdown
#### ~~R1.3 - Reference Video Intelligence~~ DONE
**Priority:** P0 - Critical
**Effort:** Medium
**Status:** Complete (2026-02-15)
**Why:** This is the core differentiator. "Drawing influence from existing TikTok videos" is the stated customer need. The `video_url` input field exists but does nothing today.

- [x] Video analysis agent: Download and analyze reference TikTok videos — VideoAnalysisAgent uses yt-dlp + Google Gemini 2.5 Flash with SEAL method
- [x] Extract pacing, hook style, energy arc, visual composition from reference — SEAL framework (Scene, Emotion, Angle, Lighting) per segment
- [x] Feed reference analysis into ScriptingAgent (match proven viral patterns) — SEAL data in prompt with strong influence instruction
- [x] Feed reference analysis into DirectorAgent (match camera work, transitions) — SEAL data enriches CastingAgent keyframe prompts
- [x] UI: Show reference video alongside generated output for comparison — Analysis review page + completed state comparison
```

**Step 2: Full build verification**

Run: `npm run build`
Expected: Compiles cleanly with zero errors

**Step 3: Commit**

```bash
git add docs/PRODUCT_ROADMAP.md
git commit -m "docs: mark R1.3 Reference Video Intelligence as complete"
git push
```

---

## Environment Setup Checklist

Before running the pipeline with video analysis:

1. **Get a Google API key:**
   - Go to https://aistudio.google.com/apikey
   - Create a new API key
   - Set `GOOGLE_API_KEY` on both Vercel and Railway

2. **Install yt-dlp on Railway worker:**
   - Add to Railway Dockerfile/Nixpacks: `apt-get install -y yt-dlp` (or `pip install yt-dlp`)
   - Verify: `yt-dlp --version` runs successfully on Railway

3. **Verify Supabase Storage:**
   - The `assets` bucket should already exist (used for product images)
   - Test: upload a small file to `assets/projects/test/reference.mp4`
